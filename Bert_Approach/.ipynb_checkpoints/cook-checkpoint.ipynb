{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2ded98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb51f76d827945a4b27b91713553f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931d4d635e914197ae8b3c0c636089d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhexplode2021\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/extend/Programming/Python/Bert_Approach/wandb/run-20231210_200457-le1abb8g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hexplode2021/huggingface/runs/le1abb8g' target=\"_blank\">swift-firefly-44</a></strong> to <a href='https://wandb.ai/hexplode2021/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hexplode2021/huggingface' target=\"_blank\">https://wandb.ai/hexplode2021/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hexplode2021/huggingface/runs/le1abb8g' target=\"_blank\">https://wandb.ai/hexplode2021/huggingface/runs/le1abb8g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='7330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  87/7330 00:05 < 08:21, 14.44 it/s, Epoch 0.12/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     57\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     59\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     60\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset_train,\n\u001b[1;32m     61\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset_eval  \u001b[38;5;66;03m# Evaluation data\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m dump(labelencoder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelencoder.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     66\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With evaluation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "import torch\n",
    "\n",
    "# 准备数据\n",
    "def prepare_data_for_bert(df):\n",
    "    df['bert_input'] = \"Text: \" + df['text'].astype(str)\n",
    "    return df\n",
    "\n",
    "# 读取数据\n",
    "df_train = pd.read_csv('bert_train_gpt.csv')  # 请确保你的 CSV 文件中有 'text' 和 'output' 这两列\n",
    "df_eval = pd.read_csv('bert_test_gpt.csv')  # Evaluation data\n",
    "\n",
    "# Label encoding for the output labels\n",
    "labelencoder = LabelEncoder()\n",
    "df_train['output'] = labelencoder.fit_transform(df_train['output'])\n",
    "df_eval['output'] = labelencoder.transform(df_eval['output']) \n",
    "\n",
    "prepared_df_train = prepare_data_for_bert(df_train)\n",
    "prepared_df_eval = prepare_data_for_bert(df_eval)\n",
    "\n",
    "# 转换成 Hugging Face Dataset\n",
    "dataset_train = Dataset.from_pandas(prepared_df_train)\n",
    "dataset_eval = Dataset.from_pandas(prepared_df_eval)\n",
    "\n",
    "# 初始化模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('./bert-base-uncased', num_labels=len(labelencoder.classes_))\n",
    "\n",
    "# 分词\n",
    "def tokenize_and_add_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch['bert_input'], truncation=True, padding='max_length', max_length=128)\n",
    "    tokenized_inputs['labels'] = batch['output']\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 应用分词\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_and_add_labels, batched=True)\n",
    "tokenized_dataset_eval = dataset_eval.map(tokenize_and_add_labels, batched=True)\n",
    "\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model every 'logging_steps'\n",
    ")\n",
    "\n",
    "# 创建 Trainer 实例并训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_eval  # Evaluation data\n",
    ")\n",
    "\n",
    "dump(labelencoder, 'labelencoder.joblib')\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./results/trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b964fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>True Positive Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>class_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2GIG Technologies</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2Wire Inc</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADT Security Services</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APC by Schneider Electric</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRIS</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASDF Technologies</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASRock</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASUS</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVM</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3083.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abode</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           True Positive  False Positive  False Negative  \\\n",
       "2GIG Technologies                    0.0             0.0             0.0   \n",
       "2Wire Inc                            0.0             0.0             0.0   \n",
       "ADT Security Services                0.0             0.0             1.0   \n",
       "APC by Schneider Electric            0.0             0.0             0.0   \n",
       "ARRIS                                9.0             1.0             1.0   \n",
       "ASDF Technologies                    0.0             0.0             1.0   \n",
       "ASRock                               2.0             0.0             1.0   \n",
       "ASUS                                13.0             8.0             6.0   \n",
       "AVM                                  7.0             0.0             3.0   \n",
       "Abode                                0.0             0.0             0.0   \n",
       "\n",
       "                           True Negative  True Positive Rate  \\\n",
       "2GIG Technologies                 3093.0                 NaN   \n",
       "2Wire Inc                         3093.0                 NaN   \n",
       "ADT Security Services             3092.0            0.000000   \n",
       "APC by Schneider Electric         3093.0                 NaN   \n",
       "ARRIS                             3082.0            0.900000   \n",
       "ASDF Technologies                 3092.0            0.000000   \n",
       "ASRock                            3090.0            0.666667   \n",
       "ASUS                              3066.0            0.684211   \n",
       "AVM                               3083.0            0.700000   \n",
       "Abode                             3093.0                 NaN   \n",
       "\n",
       "                           False Positive Rate  False Negative Rate  \\\n",
       "2GIG Technologies                     0.000000                  NaN   \n",
       "2Wire Inc                             0.000000                  NaN   \n",
       "ADT Security Services                 0.000000             1.000000   \n",
       "APC by Schneider Electric             0.000000                  NaN   \n",
       "ARRIS                                 0.000324             0.100000   \n",
       "ASDF Technologies                     0.000000             1.000000   \n",
       "ASRock                                0.000000             0.333333   \n",
       "ASUS                                  0.002602             0.315789   \n",
       "AVM                                   0.000000             0.300000   \n",
       "Abode                                 0.000000                  NaN   \n",
       "\n",
       "                           True Negative Rate  class_count  \n",
       "2GIG Technologies                    1.000000          0.0  \n",
       "2Wire Inc                            1.000000          0.0  \n",
       "ADT Security Services                1.000000          0.0  \n",
       "APC by Schneider Electric            1.000000          0.0  \n",
       "ARRIS                                0.999676         10.0  \n",
       "ASDF Technologies                    1.000000          0.0  \n",
       "ASRock                               1.000000          2.0  \n",
       "ASUS                                 0.997398         21.0  \n",
       "AVM                                  1.000000          7.0  \n",
       "Abode                                1.000000          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Function to compute the confusion matrix\n",
    "def compute_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        confusion_matrix[t, p] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "# Function to compute rates based on the confusion matrix\n",
    "def compute_rates(confusion_matrix):\n",
    "    tp = torch.diag(confusion_matrix)\n",
    "    fp = confusion_matrix.sum(dim=0) - tp\n",
    "    fn = confusion_matrix.sum(dim=1) - tp\n",
    "    tn = confusion_matrix.sum() - (fp + fn + tp)\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    class_count = tp+fp\n",
    "    \n",
    "    return tp, fp, fn, tn, tpr, fpr, fnr, tnr, class_count\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(tokenized_dataset_eval)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "true_labels = tokenized_dataset_eval[\"labels\"]\n",
    "\n",
    "# Generate confusion matrix and compute rates\n",
    "num_classes = len(labelencoder.classes_)\n",
    "confusion_matrix = compute_confusion_matrix(true_labels, pred_labels, num_classes)\n",
    "tp, fp, fn, tn, tpr, fpr, fnr, tnr, class_count = compute_rates(confusion_matrix)\n",
    "\n",
    "# Store confusion matrix in a Pandas DataFrame\n",
    "confusion_df = pd.DataFrame(confusion_matrix.numpy(), columns=labelencoder.classes_, index=labelencoder.classes_)\n",
    "\n",
    "# Store rates in a Pandas DataFrame\n",
    "rates_df = pd.DataFrame({\n",
    "    'True Positive': tp.numpy(),\n",
    "    'False Positive': fp.numpy(),\n",
    "    'False Negative': fn.numpy(),\n",
    "    'True Negative': tn.numpy(),\n",
    "    'True Positive Rate': tpr.numpy(),\n",
    "    'False Positive Rate': fpr.numpy(),\n",
    "    'False Negative Rate': fnr.numpy(),\n",
    "    'True Negative Rate': tnr.numpy(),\n",
    "    'class_count': class_count.numpy()\n",
    "}, index=labelencoder.classes_)\n",
    "\n",
    "\n",
    "print(\"\\nRates:\")\n",
    "display(rates_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c38398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df.to_csv('./results/confusion_matrix_bert_gpt_10epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e8ce64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Apple', 'Bang & Olufsen', 'Chamberlain', 'IKEA', 'Mediabridge',\n",
       "       'Microsoft', 'Pixel Magic Systems Ltd', 'Sony', 'Vizio', 'Xiaomi',\n",
       "       'acer', 'actiontec', 'airtv', 'amazon', 'amplifi', 'apple',\n",
       "       'aqara', 'arcadyan', 'arris', 'askey', 'asus', 'athom', 'august',\n",
       "       'avm', 'awox', 'belkin', 'blink', 'bluesound', 'bose', 'bravia',\n",
       "       'buffalo', 'canon', 'cisco', 'd-link', 'dell', 'denon', 'devialet',\n",
       "       'devolo', 'directv', 'doorbird', 'drobo', 'echostar', 'ecobee',\n",
       "       'eero', 'elgato', 'eve', 'facebook', 'fibaro', 'freebox',\n",
       "       'freenas', 'gardena', 'google', 'hama', 'heatmiser', 'hikvision',\n",
       "       'hisense', 'hitron', 'homebridge', 'homee', 'hp', 'huawei',\n",
       "       'humax', 'idevices', 'ieast', 'ihome', 'ikea', 'insignia',\n",
       "       'integra', 'intel', 'jbl', 'konnected', 'koogeek', 'lacie',\n",
       "       'legrand', 'lenovo', 'leviton', 'lg', 'libratone', 'lifx',\n",
       "       'linksys', 'loewe', 'logitech', 'lutron', 'marantz', 'mediatek',\n",
       "       'medion', 'microsoft', 'mikrotik', 'mill', 'motorola', 'mysa',\n",
       "       'mystrom', 'nanoleaf', 'netatmo', 'netgear', 'nvidia', 'oneplus',\n",
       "       'onkyo', 'ooma', 'openmediavault', 'oppo', 'panasonic', 'philips',\n",
       "       'phorus', 'pioneer', 'plex', 'qnap', 'rachio', 'rainmachine',\n",
       "       'raumfeld', 'ring', 'roku', 'sagemcom', 'samsung', 'seagate',\n",
       "       'sense', 'sensi', 'sharp', 'silicondust', 'sky', 'sonoff', 'sonos',\n",
       "       'sony', 'swisscom', 'syabas', 'synology', 'tado', 'tcl',\n",
       "       'technicolor', 'technisat', 'teckin', 'telus', 'tenda', 'teufel',\n",
       "       'tivo', 'toshiba', 'tp-link', 'trendnet', 'ubiquiti', 'velux',\n",
       "       'verizon', 'vizio', 'vocolinc', 'vodafone', 'wemo', 'wiimu',\n",
       "       'wink', 'wistron', 'wiz', 'xiaomi', 'yamaha', 'yeelight', 'zte',\n",
       "       'zyxel'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab04e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output label is: IKEA\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from joblib import load\n",
    "import torch\n",
    "\n",
    "# 加载保存的模型和分词器\n",
    "model = BertForSequenceClassification.from_pretrained('./results/trained_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased')\n",
    "\n",
    "# 加载保存的 LabelEncoder\n",
    "labelencoder = load('labelencoder.joblib')\n",
    "\n",
    "# 要预测的新文本样本（替换成你自己的文本和已知输出标签）\n",
    "new_text = \"Murata Manufacturing Co., Ltd.,,homekit,TRADFRI gateway,TRADFRI gateway,\"\n",
    "\n",
    "# 数据预处理（与训练时使用的格式保持一致）\n",
    "bert_input = f\"Text: {new_text}\"\n",
    "\n",
    "# 分词\n",
    "inputs = tokenizer(bert_input, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# 模型推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# 解码预测的标签\n",
    "predicted_label = labelencoder.inverse_transform([predictions.item()])[0]\n",
    "print(f\"Predicted output label is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd89175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output label is: IKEA\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from joblib import load\n",
    "import torch\n",
    "\n",
    "# 设置设备为CPU\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# 加载保存的模型和分词器\n",
    "model = BertForSequenceClassification.from_pretrained('./results/trained_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased')\n",
    "\n",
    "# 确保模型在CPU上\n",
    "model.to(device)\n",
    "\n",
    "# 加载保存的 LabelEncoder\n",
    "labelencoder = load('labelencoder.joblib')\n",
    "\n",
    "# 要预测的新文本样本（替换成你自己的文本和已知输出标签）\n",
    "new_text = \"Murata Manufacturing Co., Ltd.,,homekit,TRADFRI gateway,TRADFRI gateway,\"\n",
    "\n",
    "# 数据预处理（与训练时使用的格式保持一致）\n",
    "bert_input = f\"Text: {new_text}\"\n",
    "\n",
    "# 分词\n",
    "inputs = tokenizer(bert_input, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# 确保输入数据在CPU上\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# 模型推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# 解码预测的标签\n",
    "predicted_label = labelencoder.inverse_transform([predictions.item()])[0]\n",
    "print(f\"Predicted output label is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0790558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = BertForSequenceClassification.from_pretrained(\"./results/trained_model\")\n",
    "\n",
    "# 创建一个 dummy 输入符合模型的输入格式\n",
    "input_ids = torch.randint(0, model.config.vocab_size, (1, 128))  # 假设输入长度为 128\n",
    "# attention_mask = torch.ones(1, 128)\n",
    "attention_mask = torch.ones(1, 128).to(torch.float32)\n",
    "dummy_input = (input_ids, attention_mask)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 导出模型\n",
    "onnx_model_path = \"./onnx/model.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, input_names=['input_ids', 'attention_mask'], \n",
    "                  output_names=['output'], opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31efb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
